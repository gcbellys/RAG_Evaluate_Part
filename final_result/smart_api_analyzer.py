#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½APIè¯„åˆ†åˆ†æžè„šæœ¬

åŠŸèƒ½ï¼š
1. çœŸæ­£è¯†åˆ«å’ŒæŽ’é™¤æ— æ•ˆçš„APIè°ƒç”¨ï¼ˆç©ºç»“æžœã€å…¨0åˆ†ç­‰ï¼‰
2. åŸºäºŽå®žé™…æœ‰æ•ˆæ•°æ®è¿›è¡ŒRAGå¢žå¼ºæ•ˆæžœåˆ†æž
3. ç”Ÿæˆå‡†ç¡®çš„åˆ†æžæŠ¥å‘Š
"""

import json
import os
import glob
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Any, Tuple
import statistics

class SmartAPIScoreAnalyzer:
    """æ™ºèƒ½APIè¯„åˆ†åˆ†æžå™¨"""
    
    def __init__(self, base_dir: str = "."):
        """åˆå§‹åŒ–"""
        self.base_dir = Path(base_dir)
        self.apis = ["moonshot", "deepseek", "openai", "anthropic", "gemini"]
        
        # å­˜å‚¨æ¯ä¸ªAPIçš„è¯„åˆ†æ•°æ®
        self.api_scores = {api: {
            "baseline_scores": [],
            "rag_scores": [],
            "improvements": [],
            "declines": [],
            "no_change": [],
            "valid_reports": [],
            "invalid_reports": [],
            "empty_results": [],
            "zero_scores": []
        } for api in self.apis}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {}
        
    def load_comparison_data(self) -> Dict[str, Dict]:
        """åŠ è½½comparisonæ•°æ®"""
        print("ðŸ“ æ­£åœ¨åŠ è½½comparisonæ•°æ®...")
        comparison_dir = self.base_dir / "rerun_comparisons"
        comparison_data = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰comparisonæ–‡ä»¶
        for file_path in comparison_dir.glob("*_comparison_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ä»Žæ–‡ä»¶åæå–report_id
                filename = file_path.stem
                if "report_" in filename:
                    parts = filename.split("_")
                    report_id = parts[1]  # report_4000_comparison_timestamp
                    comparison_data[f"diagnostic_{report_id}"] = data
                    print(f"  âœ… åŠ è½½comparison: diagnostic_{report_id}")
                
            except Exception as e:
                print(f"  âŒ åŠ è½½comparisonå¤±è´¥: {file_path} - {e}")
        
        print(f"ðŸ“Š å…±åŠ è½½ {len(comparison_data)} ä¸ªcomparisonæ–‡ä»¶")
        return comparison_data
    
    def is_api_call_valid(self, api_data: Dict) -> Tuple[bool, str]:
        """åˆ¤æ–­APIè°ƒç”¨æ˜¯å¦çœŸæ­£æœ‰æ•ˆ"""
        # æ£€æŸ¥å™¨å®˜åç§°æ˜¯å¦ä¸ºç©º
        baseline_organ = api_data.get("baseline_organ", "")
        rag_organ = api_data.get("rag_organ", "")
        
        if not baseline_organ or not rag_organ:
            return False, "å™¨å®˜åç§°ä¸ºç©º"
        
        # æ£€æŸ¥ä½ç½®æ•°ç»„æ˜¯å¦ä¸ºç©º
        baseline_locations = api_data.get("baseline_locations", [])
        rag_locations = api_data.get("rag_locations", [])
        
        if not baseline_locations and not rag_locations:
            return False, "ä½ç½®æ•°ç»„ä¸ºç©º"
        
        # æ£€æŸ¥metricsæ˜¯å¦å­˜åœ¨
        baseline_metrics = api_data.get("baseline_metrics", {})
        rag_metrics = api_data.get("rag_metrics", {})
        
        if not baseline_metrics or not rag_metrics:
            return False, "metricsæ•°æ®ç¼ºå¤±"
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æ•°éƒ½ä¸º0
        baseline_score = baseline_metrics.get("overall_score", 0)
        rag_score = rag_metrics.get("overall_score", 0)
        
        if baseline_score == 0 and rag_score == 0:
            return False, "æ‰€æœ‰åˆ†æ•°éƒ½ä¸º0"
        
        # æ£€æŸ¥å™¨å®˜è¯†åˆ«æ˜¯å¦å®Œå…¨å¤±è´¥
        baseline_organ_accuracy = api_data.get("baseline_organ_accuracy", {})
        rag_organ_accuracy = api_data.get("rag_organ_accuracy", {})
        
        if (baseline_organ_accuracy.get("description", "") == "æœªè¯†åˆ«å‡ºä»»ä½•å™¨å®˜" and 
            rag_organ_accuracy.get("description", "") == "æœªè¯†åˆ«å‡ºä»»ä½•å™¨å®˜"):
            return False, "å®Œå…¨æœªè¯†åˆ«å‡ºå™¨å®˜"
        
        return True, "æœ‰æ•ˆ"
    
    def extract_api_scores(self, comparison_data: Dict[str, Dict]):
        """æå–æ¯ä¸ªAPIçš„è¯„åˆ†æ•°æ®ï¼ŒçœŸæ­£æŽ’é™¤æ— æ•ˆæ•°æ®"""
        print("ðŸ”„ æ­£åœ¨æå–APIè¯„åˆ†æ•°æ®ï¼ˆæ™ºèƒ½æŽ’é™¤æ— æ•ˆæ•°æ®ï¼‰...")
        
        for report_id, report_data in comparison_data.items():
            print(f"\nðŸ“‹ å¤„ç†æŠ¥å‘Š: {report_id}")
            
            # èŽ·å–ç—‡çŠ¶åˆ—è¡¨ - è¿™é‡Œæ¯ä¸ªç—‡çŠ¶æ˜¯ä¸€ä¸ªé”®å€¼å¯¹
            symptoms = report_data
            
            for symptom_name, symptom_data in symptoms.items():
                if isinstance(symptom_data, dict) and any(api in symptom_data for api in self.apis):
                    print(f"  å¤„ç†ç—‡çŠ¶: {symptom_name}")
                    
                    # å¤„ç†æ¯ä¸ªAPIçš„è¯„åˆ†
                    for api in self.apis:
                        if api in symptom_data:
                            self._process_api_scores(api, symptom_data[api], symptom_name, report_id)
    
    def _process_api_scores(self, api: str, symptom_data: Dict, symptom_name: str, report_id: str):
        """å¤„ç†å•ä¸ªAPIçš„è¯„åˆ†ï¼Œæ™ºèƒ½æŽ’é™¤æ— æ•ˆæ•°æ®"""
        # é¦–å…ˆåˆ¤æ–­APIè°ƒç”¨æ˜¯å¦çœŸæ­£æœ‰æ•ˆ
        is_valid, reason = self.is_api_call_valid(symptom_data)
        
        if not is_valid:
            # è®°å½•æ— æ•ˆæ•°æ®
            self.api_scores[api]["invalid_reports"].append(report_id)
            
            # åˆ†ç±»è®°å½•ä¸åŒç±»åž‹çš„æ— æ•ˆæ•°æ®
            if "å™¨å®˜åç§°ä¸ºç©º" in reason or "ä½ç½®æ•°ç»„ä¸ºç©º" in reason:
                self.api_scores[api]["empty_results"].append(report_id)
            elif "æ‰€æœ‰åˆ†æ•°éƒ½ä¸º0" in reason:
                self.api_scores[api]["zero_scores"].append(report_id)
            
            print(f"    {api}: {reason} (å·²æŽ’é™¤)")
            return
        
        # èŽ·å–è¯„åˆ†æŒ‡æ ‡
        baseline_metrics = symptom_data.get("baseline_metrics", {})
        rag_metrics = symptom_data.get("rag_metrics", {})
        
        baseline_score = baseline_metrics.get("overall_score", 0)
        rag_score = rag_metrics.get("overall_score", 0)
        
        # å­˜å‚¨æœ‰æ•ˆæ•°æ®
        self.api_scores[api]["baseline_scores"].append(baseline_score)
        self.api_scores[api]["rag_scores"].append(rag_score)
        self.api_scores[api]["valid_reports"].append(report_id)
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æå‡
        if rag_score > baseline_score:
            self.api_scores[api]["improvements"].append(rag_score - baseline_score)
        elif rag_score < baseline_score:
            self.api_scores[api]["declines"].append(baseline_score - rag_score)
        else:
            self.api_scores[api]["no_change"].append(0)
        
        print(f"    {api}: baseline={baseline_score}, RAG={rag_score} (æœ‰æ•ˆ)")
    
    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        print("\nðŸ“Š æ­£åœ¨è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
        
        for api in self.apis:
            baseline_scores = self.api_scores[api]["baseline_scores"]
            rag_scores = self.api_scores[api]["rag_scores"]
            improvements = self.api_scores[api]["improvements"]
            declines = self.api_scores[api]["declines"]
            no_change = self.api_scores[api]["no_change"]
            valid_reports = self.api_scores[api]["valid_reports"]
            invalid_reports = self.api_scores[api]["invalid_reports"]
            empty_results = self.api_scores[api]["empty_results"]
            zero_scores = self.api_scores[api]["zero_scores"]
            
            if not baseline_scores:
                print(f"  âš ï¸  {api}: æ— æœ‰æ•ˆæ•°æ®")
                continue
            
            total_symptoms = len(baseline_scores)
            improvement_count = len(improvements)
            decline_count = len(declines)
            no_change_count = len(no_change)
            unique_valid_reports = len(set(valid_reports))
            unique_invalid_reports = len(set(invalid_reports))
            unique_empty_results = len(set(empty_results))
            unique_zero_scores = len(set(zero_scores))
            
            # è®¡ç®—å¹³å‡åˆ†
            baseline_avg = statistics.mean(baseline_scores) if baseline_scores else 0
            rag_avg = statistics.mean(rag_scores) if rag_scores else 0
            
            # è®¡ç®—æ¯”ä¾‹
            improvement_ratio = improvement_count / total_symptoms if total_symptoms > 0 else 0
            decline_ratio = decline_count / total_symptoms if total_symptoms > 0 else 0
            no_change_ratio = no_change_count / total_symptoms if total_symptoms > 0 else 0
            
            self.stats[api] = {
                "s_numbers": total_symptoms,
                "valid_reports_count": unique_valid_reports,
                "invalid_reports_count": unique_invalid_reports,
                "empty_results_count": unique_empty_results,
                "zero_scores_count": unique_zero_scores,
                "scores_without_rag": {
                    "overall_score": round(baseline_avg, 2),
                    "total_scores": baseline_scores
                },
                "scores_with_rag": {
                    "overall_score": round(rag_avg, 2),
                    "total_scores": rag_scores
                },
                "improvement_ratio": round(improvement_ratio * 100, 2),
                "decline_ratio": round(decline_ratio * 100, 2),
                "no_change_ratio": round(no_change_ratio * 100, 2),
                "improvement_count": improvement_count,
                "decline_count": decline_count,
                "no_change_count": no_change_count,
                "data_quality": {
                    "valid_symptoms": total_symptoms,
                    "invalid_reports": unique_invalid_reports,
                    "empty_results": unique_empty_results,
                    "zero_scores": unique_zero_scores,
                    "data_completeness": f"{unique_valid_reports}/{unique_valid_reports + unique_invalid_reports}"
                }
            }
            
            print(f"  âœ… {api}: {total_symptoms}ä¸ªæœ‰æ•ˆç—‡çŠ¶, æå‡çŽ‡{improvement_ratio*100:.1f}%, ä¸‹é™çŽ‡{decline_ratio*100:.1f}%")
            if unique_invalid_reports > 0:
                print(f"     âš ï¸  å‘çŽ°{unique_invalid_reports}ä¸ªæ— æ•ˆæŠ¥å‘Šï¼Œå·²æŽ’é™¤")
                if unique_empty_results > 0:
                    print(f"       å…¶ä¸­{unique_empty_results}ä¸ªä¸ºç©ºç»“æžœ")
                if unique_zero_scores > 0:
                    print(f"       å…¶ä¸­{unique_zero_scores}ä¸ªä¸ºå…¨0åˆ†")
    
    def generate_report(self, output_file: str = "smart_api_analysis_report.json"):
        """ç”Ÿæˆåˆ†æžæŠ¥å‘Š"""
        print(f"\nðŸ“ æ­£åœ¨ç”Ÿæˆæ™ºèƒ½åˆ†æžæŠ¥å‘Š: {output_file}")
        
        # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
        detailed_report = {
            "analysis_timestamp": str(Path.cwd()),
            "total_apis": len(self.apis),
            "data_quality_note": "å·²æ™ºèƒ½æŽ’é™¤ç©ºç»“æžœã€å…¨0åˆ†ç­‰æ— æ•ˆAPIè°ƒç”¨",
            "api_statistics": self.stats,
            "summary": {
                "total_symptoms_processed": sum(stat["s_numbers"] for stat in self.stats.values()),
                "apis_with_valid_data": [api for api, stat in self.stats.items() if stat["s_numbers"] > 0],
                "best_improvement_api": max(self.stats.items(), key=lambda x: x[1]["improvement_ratio"])[0] if self.stats else None,
                "worst_decline_api": max(self.stats.items(), key=lambda x: x[1]["decline_ratio"])[0] if self.stats else None,
                "most_stable_api": max(self.stats.items(), key=lambda x: x[1]["no_change_ratio"])[0] if self.stats else None
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç”Ÿæˆäººç±»å¯è¯»çš„æ‘˜è¦
        self._generate_human_readable_summary()
    
    def _generate_human_readable_summary(self):
        """ç”Ÿæˆäººç±»å¯è¯»çš„æ‘˜è¦"""
        print("\n" + "="*80)
        print("ðŸŽ¯ æ™ºèƒ½APIè¯„åˆ†åˆ†æžæ‘˜è¦")
        print("="*80)
        
        for api, stat in self.stats.items():
            if stat["s_numbers"] == 0:
                continue
                
            print(f"\nðŸ“Š {api.upper()}:")
            print(f"  æœ‰æ•ˆç—‡çŠ¶æ€»æ•°: {stat['s_numbers']}")
            print(f"  æœ‰æ•ˆæŠ¥å‘Šæ•°é‡: {stat['data_quality']['valid_symptoms']}")
            print(f"  æ— æ•ˆæŠ¥å‘Šæ•°é‡: {stat['data_quality']['invalid_reports']}")
            if stat['data_quality']['empty_results'] > 0:
                print(f"    å…¶ä¸­ç©ºç»“æžœ: {stat['data_quality']['empty_results']}")
            if stat['data_quality']['zero_scores'] > 0:
                print(f"    å…¶ä¸­å…¨0åˆ†: {stat['data_quality']['zero_scores']}")
            print(f"  æ— RAGå¹³å‡åˆ†: {stat['scores_without_rag']['overall_score']}")
            print(f"  æœ‰RAGå¹³å‡åˆ†: {stat['scores_with_rag']['overall_score']}")
            print(f"  RAGæå‡æ¯”ä¾‹: {stat['improvement_ratio']}% ({stat['improvement_count']}ä¸ª)")
            print(f"  RAGä¸‹é™æ¯”ä¾‹: {stat['decline_ratio']}% ({stat['decline_count']}ä¸ª)")
            print(f"  æ— å˜åŒ–æ¯”ä¾‹: {stat['no_change_ratio']}% ({stat['no_change_count']}ä¸ª)")
            
            # è®¡ç®—å¹³å‡æå‡/ä¸‹é™
            if stat['improvement_count'] > 0:
                improvements = []
                for i, (rag_score, baseline_score) in enumerate(zip(stat['scores_with_rag']['total_scores'], 
                                                                   stat['scores_without_rag']['total_scores'])):
                    if rag_score > baseline_score:
                        improvements.append(rag_score - baseline_score)
                
                if improvements:
                    avg_improvement = statistics.mean(improvements)
                    print(f"  å¹³å‡æå‡åˆ†æ•°: {avg_improvement:.2f}")
            
            if stat['decline_count'] > 0:
                declines = []
                for i, (rag_score, baseline_score) in enumerate(zip(stat['scores_with_rag']['total_scores'], 
                                                                   stat['scores_without_rag']['total_scores'])):
                    if rag_score < baseline_score:
                        declines.append(baseline_score - rag_score)
                
                if declines:
                    avg_decline = statistics.mean(declines)
                    print(f"  å¹³å‡ä¸‹é™åˆ†æ•°: {avg_decline:.2f}")
        
        print("\n" + "="*80)
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æž"""
        print("ðŸš€ å¼€å§‹æ™ºèƒ½APIè¯„åˆ†åˆ†æž...")
        
        # 1. åŠ è½½æ•°æ®
        comparison_data = self.load_comparison_data()
        
        # 2. æå–è¯„åˆ†
        self.extract_api_scores(comparison_data)
        
        # 3. è®¡ç®—ç»Ÿè®¡
        self.calculate_statistics()
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        print("\nâœ… æ™ºèƒ½APIè¯„åˆ†åˆ†æžå®Œæˆï¼")
        return self.stats

def main():
    """ä¸»å‡½æ•°"""
    analyzer = SmartAPIScoreAnalyzer()
    stats = analyzer.run_analysis()
    
    # è¿”å›žç»“æžœä¾›å…¶ä»–è„šæœ¬ä½¿ç”¨
    return stats

if __name__ == "__main__":
    main()
