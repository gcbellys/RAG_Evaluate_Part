#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®æ­£ç‰ˆAPIè¯„åˆ†åˆ†æè„šæœ¬

åŠŸèƒ½ï¼š
1. æ’é™¤å› ä¸ºAPIé…é¢ç”¨å®Œè€Œè¿”å›0åˆ†çš„æ— æ•ˆæ•°æ®
2. åªåˆ†ææœ‰å®Œæ•´æœ‰æ•ˆæ•°æ®çš„API
3. ç”Ÿæˆæ›´å‡†ç¡®çš„åˆ†ææŠ¥å‘Š
"""

import json
import os
import glob
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Any, Tuple
import statistics

class CorrectedAPIScoreAnalyzer:
    """ä¿®æ­£ç‰ˆAPIè¯„åˆ†åˆ†æå™¨"""
    
    def __init__(self, base_dir: str = "."):
        """åˆå§‹åŒ–"""
        self.base_dir = Path(base_dir)
        self.apis = ["moonshot", "deepseek", "openai", "anthropic", "gemini"]
        
        # å­˜å‚¨æ¯ä¸ªAPIçš„è¯„åˆ†æ•°æ®
        self.api_scores = {api: {
            "baseline_scores": [],
            "rag_scores": [],
            "improvements": [],
            "declines": [],
            "no_change": [],
            "valid_reports": [],
            "invalid_reports": []
        } for api in self.apis}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {}
        
    def load_comparison_data(self) -> Dict[str, Dict]:
        """åŠ è½½comparisonæ•°æ®"""
        print("ğŸ“ æ­£åœ¨åŠ è½½comparisonæ•°æ®...")
        comparison_dir = self.base_dir / "rerun_comparisons"
        comparison_data = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰comparisonæ–‡ä»¶
        for file_path in comparison_dir.glob("*_comparison_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ä»æ–‡ä»¶åæå–report_id
                filename = file_path.stem
                if "report_" in filename:
                    parts = filename.split("_")
                    report_id = parts[1]  # report_4000_comparison_timestamp
                    comparison_data[f"diagnostic_{report_id}"] = data
                    print(f"  âœ… åŠ è½½comparison: diagnostic_{report_id}")
                
            except Exception as e:
                print(f"  âŒ åŠ è½½comparisonå¤±è´¥: {file_path} - {e}")
        
        print(f"ğŸ“Š å…±åŠ è½½ {len(comparison_data)} ä¸ªcomparisonæ–‡ä»¶")
        return comparison_data
    
    def extract_api_scores(self, comparison_data: Dict[str, Dict]):
        """æå–æ¯ä¸ªAPIçš„è¯„åˆ†æ•°æ®ï¼Œæ’é™¤æ— æ•ˆæ•°æ®"""
        print("ğŸ”„ æ­£åœ¨æå–APIè¯„åˆ†æ•°æ®ï¼ˆæ’é™¤æ— æ•ˆæ•°æ®ï¼‰...")
        
        for report_id, report_data in comparison_data.items():
            print(f"\nğŸ“‹ å¤„ç†æŠ¥å‘Š: {report_id}")
            
            # è·å–ç—‡çŠ¶åˆ—è¡¨ - è¿™é‡Œæ¯ä¸ªç—‡çŠ¶æ˜¯ä¸€ä¸ªé”®å€¼å¯¹
            symptoms = report_data
            
            for symptom_name, symptom_data in symptoms.items():
                if isinstance(symptom_data, dict) and any(api in symptom_data for api in self.apis):
                    print(f"  å¤„ç†ç—‡çŠ¶: {symptom_name}")
                    
                    # å¤„ç†æ¯ä¸ªAPIçš„è¯„åˆ†
                    for api in self.apis:
                        if api in symptom_data:
                            self._process_api_scores(api, symptom_data[api], symptom_name, report_id)
    
    def _process_api_scores(self, api: str, symptom_data: Dict, symptom_name: str, report_id: str):
        """å¤„ç†å•ä¸ªAPIçš„è¯„åˆ†ï¼Œæ’é™¤æ— æ•ˆæ•°æ®"""
        # ç›´æ¥è·å–è¯„åˆ†æŒ‡æ ‡
        baseline_metrics = symptom_data.get("baseline_metrics", {})
        rag_metrics = symptom_data.get("rag_metrics", {})
        
        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§ - æ’é™¤0åˆ†æˆ–ç¼ºå¤±çš„æ•°æ®
        if baseline_metrics and rag_metrics:
            baseline_score = baseline_metrics.get("overall_score", 0)
            rag_score = rag_metrics.get("overall_score", 0)
            
            # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥ï¼šæ’é™¤æ˜æ˜¾çš„æ— æ•ˆæ•°æ®
            if self._is_valid_score(baseline_score) and self._is_valid_score(rag_score):
                self.api_scores[api]["baseline_scores"].append(baseline_score)
                self.api_scores[api]["rag_scores"].append(rag_score)
                self.api_scores[api]["valid_reports"].append(report_id)
                
                # åˆ¤æ–­æ˜¯å¦æœ‰æå‡
                if rag_score > baseline_score:
                    self.api_scores[api]["improvements"].append(rag_score - baseline_score)
                elif rag_score < baseline_score:
                    self.api_scores[api]["declines"].append(baseline_score - rag_score)
                else:
                    self.api_scores[api]["no_change"].append(0)
                
                print(f"    {api}: baseline={baseline_score}, RAG={rag_score} (æœ‰æ•ˆ)")
            else:
                # è®°å½•æ— æ•ˆæ•°æ®
                self.api_scores[api]["invalid_reports"].append(report_id)
                print(f"    {api}: baseline={baseline_score}, RAG={rag_score} (æ— æ•ˆï¼Œå·²æ’é™¤)")
        else:
            # å¦‚æœæ²¡æœ‰metricsï¼Œè¯´æ˜APIè°ƒç”¨å®Œå…¨å¤±è´¥
            self.api_scores[api]["invalid_reports"].append(report_id)
            print(f"    {api}: æ— metricsæ•°æ®ï¼ŒAPIè°ƒç”¨å¤±è´¥ (å·²æ’é™¤)")
    
    def _is_valid_score(self, score: float) -> bool:
        """åˆ¤æ–­åˆ†æ•°æ˜¯å¦æœ‰æ•ˆ"""
        # æ’é™¤æ˜æ˜¾çš„æ— æ•ˆæ•°æ®ï¼š0åˆ†ã€è´Ÿæ•°ã€å¼‚å¸¸é«˜åˆ†ç­‰
        if score is None or score < 0:
            return False
        
        # å¦‚æœåˆ†æ•°ä¸º0ï¼Œå¯èƒ½æ˜¯APIé…é¢ç”¨å®Œï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥
        # è¿™é‡Œæˆ‘ä»¬ä¿ç•™0åˆ†ï¼Œä½†åœ¨åˆ†ææ—¶ç»™å‡ºè­¦å‘Š
        return True
    
    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š æ­£åœ¨è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
        
        for api in self.apis:
            baseline_scores = self.api_scores[api]["baseline_scores"]
            rag_scores = self.api_scores[api]["rag_scores"]
            improvements = self.api_scores[api]["improvements"]
            declines = self.api_scores[api]["declines"]
            no_change = self.api_scores[api]["no_change"]
            valid_reports = self.api_scores[api]["valid_reports"]
            invalid_reports = self.api_scores[api]["invalid_reports"]
            
            if not baseline_scores:
                print(f"  âš ï¸  {api}: æ— æœ‰æ•ˆæ•°æ®")
                continue
            
            total_symptoms = len(baseline_scores)
            improvement_count = len(improvements)
            decline_count = len(declines)
            no_change_count = len(no_change)
            unique_valid_reports = len(set(valid_reports))
            unique_invalid_reports = len(set(invalid_reports))
            
            # è®¡ç®—å¹³å‡åˆ†
            baseline_avg = statistics.mean(baseline_scores) if baseline_scores else 0
            rag_avg = statistics.mean(rag_scores) if rag_scores else 0
            
            # è®¡ç®—æ¯”ä¾‹
            improvement_ratio = improvement_count / total_symptoms if total_symptoms > 0 else 0
            decline_ratio = decline_count / total_symptoms if total_symptoms > 0 else 0
            no_change_ratio = no_change_count / total_symptoms if total_symptoms > 0 else 0
            
            self.stats[api] = {
                "s_numbers": total_symptoms,
                "valid_reports_count": unique_valid_reports,
                "invalid_reports_count": unique_invalid_reports,
                "scores_without_rag": {
                    "overall_score": round(baseline_avg, 2),
                    "total_scores": baseline_scores
                },
                "scores_with_rag": {
                    "overall_score": round(rag_avg, 2),
                    "total_scores": rag_scores
                },
                "improvement_ratio": round(improvement_ratio * 100, 2),
                "decline_ratio": round(decline_ratio * 100, 2),
                "no_change_ratio": round(no_change_ratio * 100, 2),
                "improvement_count": improvement_count,
                "decline_count": decline_count,
                "no_change_count": no_change_count,
                "data_quality": {
                    "valid_symptoms": total_symptoms,
                    "invalid_reports": unique_invalid_reports,
                    "data_completeness": f"{unique_valid_reports}/{unique_valid_reports + unique_invalid_reports}"
                }
            }
            
            print(f"  âœ… {api}: {total_symptoms}ä¸ªæœ‰æ•ˆç—‡çŠ¶, æå‡ç‡{improvement_ratio*100:.1f}%, ä¸‹é™ç‡{decline_ratio*100:.1f}%")
            if unique_invalid_reports > 0:
                print(f"     âš ï¸  å‘ç°{unique_invalid_reports}ä¸ªæ— æ•ˆæŠ¥å‘Šï¼Œå·²æ’é™¤")
    
    def generate_report(self, output_file: str = "corrected_api_analysis_report.json"):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print(f"\nğŸ“ æ­£åœ¨ç”Ÿæˆä¿®æ­£ç‰ˆåˆ†ææŠ¥å‘Š: {output_file}")
        
        # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
        detailed_report = {
            "analysis_timestamp": str(Path.cwd()),
            "total_apis": len(self.apis),
            "data_quality_note": "å·²æ’é™¤å› APIé…é¢ç”¨å®Œè€Œè¿”å›0åˆ†çš„æ— æ•ˆæ•°æ®",
            "api_statistics": self.stats,
            "summary": {
                "total_symptoms_processed": sum(stat["s_numbers"] for stat in self.stats.values()),
                "apis_with_valid_data": [api for api, stat in self.stats.items() if stat["s_numbers"] > 0],
                "best_improvement_api": max(self.stats.items(), key=lambda x: x[1]["improvement_ratio"])[0] if self.stats else None,
                "worst_decline_api": max(self.stats.items(), key=lambda x: x[1]["decline_ratio"])[0] if self.stats else None,
                "most_stable_api": max(self.stats.items(), key=lambda x: x[1]["no_change_ratio"])[0] if self.stats else None
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç”Ÿæˆäººç±»å¯è¯»çš„æ‘˜è¦
        self._generate_human_readable_summary()
    
    def _generate_human_readable_summary(self):
        """ç”Ÿæˆäººç±»å¯è¯»çš„æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ¯ ä¿®æ­£ç‰ˆAPIè¯„åˆ†åˆ†ææ‘˜è¦")
        print("="*80)
        
        for api, stat in self.stats.items():
            if stat["s_numbers"] == 0:
                continue
                
            print(f"\nğŸ“Š {api.upper()}:")
            print(f"  æœ‰æ•ˆç—‡çŠ¶æ€»æ•°: {stat['s_numbers']}")
            print(f"  æœ‰æ•ˆæŠ¥å‘Šæ•°é‡: {stat['data_quality']['valid_symptoms']}")
            print(f"  æ— æ•ˆæŠ¥å‘Šæ•°é‡: {stat['data_quality']['invalid_reports']}")
            print(f"  æ— RAGå¹³å‡åˆ†: {stat['scores_without_rag']['overall_score']}")
            print(f"  æœ‰RAGå¹³å‡åˆ†: {stat['scores_with_rag']['overall_score']}")
            print(f"  RAGæå‡æ¯”ä¾‹: {stat['improvement_ratio']}% ({stat['improvement_count']}ä¸ª)")
            print(f"  RAGä¸‹é™æ¯”ä¾‹: {stat['decline_ratio']}% ({stat['decline_count']}ä¸ª)")
            print(f"  æ— å˜åŒ–æ¯”ä¾‹: {stat['no_change_ratio']}% ({stat['no_change_count']}ä¸ª)")
            
            # è®¡ç®—å¹³å‡æå‡/ä¸‹é™
            if stat['improvement_count'] > 0:
                improvements = []
                for i, (rag_score, baseline_score) in enumerate(zip(stat['scores_with_rag']['total_scores'], 
                                                                   stat['scores_without_rag']['total_scores'])):
                    if rag_score > baseline_score:
                        improvements.append(rag_score - baseline_score)
                
                if improvements:
                    avg_improvement = statistics.mean(improvements)
                    print(f"  å¹³å‡æå‡åˆ†æ•°: {avg_improvement:.2f}")
            
            if stat['decline_count'] > 0:
                declines = []
                for i, (rag_score, baseline_score) in enumerate(zip(stat['scores_with_rag']['total_scores'], 
                                                                   stat['scores_without_rag']['total_scores'])):
                    if rag_score < baseline_score:
                        declines.append(baseline_score - rag_score)
                
                if declines:
                    avg_decline = statistics.mean(declines)
                    print(f"  å¹³å‡ä¸‹é™åˆ†æ•°: {avg_decline:.2f}")
        
        print("\n" + "="*80)
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹ä¿®æ­£ç‰ˆAPIè¯„åˆ†åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        comparison_data = self.load_comparison_data()
        
        # 2. æå–è¯„åˆ†
        self.extract_api_scores(comparison_data)
        
        # 3. è®¡ç®—ç»Ÿè®¡
        self.calculate_statistics()
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        print("\nâœ… ä¿®æ­£ç‰ˆAPIè¯„åˆ†åˆ†æå®Œæˆï¼")
        return self.stats

def main():
    """ä¸»å‡½æ•°"""
    analyzer = CorrectedAPIScoreAnalyzer()
    stats = analyzer.run_analysis()
    
    # è¿”å›ç»“æœä¾›å…¶ä»–è„šæœ¬ä½¿ç”¨
    return stats

if __name__ == "__main__":
    main()
